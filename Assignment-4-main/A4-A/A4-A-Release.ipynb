{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT6Ci5F2YY0U"
   },
   "source": [
    "# Introduction to Neural Networks - MicroTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural networks are a core component of deep learning, driving advancements in image recognition, natural language processing, robotics, and more. These networks are made up of interconnected layers that transform data through a series of operations, allowing them to learn complex patterns. At the heart of neural networks is the process of learning through gradients, calculated by *backpropagation* and the *chain rule*, which enables the network to adjust its parameters based on how much each parameter contributes to the overall error.\n",
    "\n",
    "Modern deep learning frameworks like **PyTorch** and **TensorFlow** provide powerful abstractions for building and training neural networks, automatically handling gradient calculations and optimization steps. However, to truly understand how neural networks learn, it’s invaluable to implement the core components from scratch, gaining insight into the inner workings of backpropagation.\n",
    "\n",
    "In this assignment, we will build a mini neural network framework that supports gradient propagation, allowing us to train a network on a real task and observe how it learns. Specifically, you will **implement a Neural Network from Scratch using MicroTorch**: Develop a simple neural network framework with core operations like linear transformations, activation functions, and loss calculations. Implement backpropagation and a training loop to optimize network parameters.\n",
    "\n",
    "**Notes:**\n",
    "- We've provided all the imports that you'll need for this assignment, though you're welcome to add imports _from the Python Standard Library only_ if you need them. If you want to add any helper functions for whatever reason, please make sure that they are nested inside the function that they are used in. This is to ensure that the autograder can run your code without any issues.\n",
    "- ***DO NOT REMOVE ANY COMMENTS THAT HAVE `# export` IN THEM. THE GRADING SCRIPT USES THESE COMMENTS TO EVALUATE YOUR FUNCTIONS. WE WILL NOT AUDIT SUBMISSIONS TO ADD THESE. IF THE AUTOGRADER FAILS TO RUN DUE TO YOUR MODIFICATION OF THESE COMMENTS, YOU WILL NOT RECEIVE CREDIT.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (2.2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/CS-3600/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72_ombXDbd5g"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z62QX-pqunOY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# export - DO NOT MODIFY THIS CELL\n",
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import plot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "# add any additional imports here (from the Python Standard Library only!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp_qws9jbqMD"
   },
   "source": [
    "# Neural Networks in MicroTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-ia6fBoYY0X"
   },
   "source": [
    "## MicroTorch Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djeKeIfRglvb"
   },
   "source": [
    "In this section, you’ll build a mini neural network framework called **MicroTorch**. This framework mimics some of the core functionalities of PyTorch but at a much simpler level. The goal of MicroTorch is to give you hands-on experience with backpropagation, gradient chaining, and parameter updates, helping you understand the underlying mechanics of a neural network without relying on a high-level framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjVZ8ENvF2Kz"
   },
   "source": [
    "### Why Build MicroTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern deep learning frameworks, such as PyTorch and TensorFlow, handle computations and backpropagation automatically by creating **computation chains**. In PyTorch, each `Tensor` object keeps track of operations applied to it, forming a chain of computations that allows automatic gradient calculation via **automatic differentiation**.\n",
    "\n",
    "MicroTorch takes a slightly different approach. Instead of tracking operations in `Tensor` objects, each **module** (or layer) in MicroTorch will track its own inputs, outputs, and gradients. This simplified approach allows us to implement chaining through the modules themselves, making it easier to see how data flows through the network and how gradients are propagated backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX1S1o58gt9p"
   },
   "source": [
    "### How MicroTorch Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each module in MicroTorch will keep track of:\n",
    "- **Inputs** (`self._in`): The data it received from the previous layer.\n",
    "- **Outputs** (`self._out`): The results it produced.\n",
    "- **Gradients** (`self._delta`): The computed gradients during the backward pass.\n",
    "\n",
    "By structuring MicroTorch in this way, we can implement the forward and backward passes manually, capturing inputs, outputs, and gradients at each layer. This approach lacks the optimization and scalability features of frameworks like PyTorch, but it provides valuable insight into how neural networks work under the hood.\n",
    "\n",
    "With MicroTorch, you'll:\n",
    "1. Implement custom layers and activation functions.\n",
    "2. Chain modules together to form a complete neural network.\n",
    "3. Train the network by manually calculating gradients and updating parameters.\n",
    "\n",
    "Let's dive into building MicroTorch and understanding each step in detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK7vYze2bpB3"
   },
   "source": [
    "## Abstract Base Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEtAKanBMAr4"
   },
   "source": [
    "### Module Base Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nR9W_X_HYY0Y"
   },
   "source": [
    "In MicroTorch, all layers and loss functions will inherit from a base class called `Module`. The `Module` class defines the basic structure and functionality that every layer or criterion (loss function) in our network will share.\n",
    "\n",
    "Each `Module` has the following attributes:\n",
    "- `parameters`: This will store the weights and biases for layers that require them.\n",
    "- `_in`: Stores the inputs to the layer, which are essential for calculating gradients in the backward pass.\n",
    "- `_out`: Stores the outputs from the layer after the forward pass.\n",
    "- `_delta`: Stores the gradients (deltas) computed during the backward pass.\n",
    "\n",
    "The `Module` class also includes these core methods:\n",
    "- `forward(x)`: Defines the computation for the forward pass, taking input `x`.\n",
    "- `backward(delta)`: Defines the computation for the backward pass, using `delta` from the next layer.\n",
    "- `update(lr)`: Updates the parameters (if they exist) using the learning rate `lr`.\n",
    "\n",
    "This base class simplifies the creation of new layers by ensuring they have a standard structure. Let's look at the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "THgLW7UFrCpO"
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.parameters = None  # Module parameters stored here\n",
    "        self._delta = None  # Gradients saved from chaining backward()\n",
    "        self._out = None  # Outputs of forward() saved for chaining\n",
    "        self._in = None  # Inputs to forward() saved for chaining\n",
    "\n",
    "    # Call to run the module during the forward pass\n",
    "    # x is the numpy array containing (batched) inputs\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    # Call to run the module during the backward pass\n",
    "    # Delta is the gradient from the previous module's backward call.\n",
    "    def backward(self, delta=1.0):\n",
    "        pass\n",
    "\n",
    "    # Call to update parameters\n",
    "    # lr is the learning rate\n",
    "    def update(self, lr):\n",
    "        if self.parameters is not None:\n",
    "            # print(\"dot: \", np.dot(self._in.T, self._delta))\n",
    "            # print(\"param\", self.parameters)\n",
    "            self.parameters = self.parameters - (lr * np.dot(self._in.T, self._delta))\n",
    "\n",
    "    # This makes it so that the object reference be called like a function.\n",
    "    # When doing so, the forward function is automatically called.\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LajJzNSM2kn"
   },
   "source": [
    "### Criterion Base Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g47NJtI3YY0Z"
   },
   "source": [
    "The `Criterion` class is a specialized type of `Module` used for defining loss functions. Like `Module`, `Criterion` has a `forward` and `backward` function, but with slight differences:\n",
    "- The `forward` function now takes two arguments: `y_hat` (the predictions) and `y` (the target values). This allows it to compute the loss between the predicted and actual outputs.\n",
    "- The `backward` function will calculate the gradient of the loss with respect to the predictions `y_hat`.\n",
    "\n",
    "This setup ensures a consistent structure for all loss functions, making it easy to add custom loss criteria as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KEshF2Dgzaid"
   },
   "outputs": [],
   "source": [
    "class Criterion(Module):\n",
    "    def __init__(self):\n",
    "        super(Criterion, self).__init__()\n",
    "        self._target = None  # Remember the target when forward is called\n",
    "\n",
    "    # The forward function now takes two inputs:\n",
    "    # y_hat is the prediction to be compared against the target\n",
    "    # y is the target\n",
    "    def forward(self, y_hat, y):\n",
    "        pass\n",
    "\n",
    "    # This makes it so that the object reference be called like a function.\n",
    "    # When doing so, the forward function is automatically called.\n",
    "    def __call__(self, x, y):\n",
    "        return self.forward(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn4jZch7KEqY"
   },
   "source": [
    "### Wrapper Functions for Forward and Backward Passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAp6W05jYY0Z"
   },
   "source": [
    "In MicroTorch, we will use **wrapper functions** to automatically capture and store inputs, outputs, and gradients during the forward and backward passes. This simplifies the process of chaining operations and makes sure the necessary information is stored in each layer without adding extra code in every function.\n",
    "\n",
    "**Why Use Wrapper Functions?**\n",
    "\n",
    "In PyTorch, each `Tensor` object keeps track of operations applied to it, so chaining and gradient tracking happen automatically. However, in MicroTorch, we’re using regular numpy arrays, so we need to store this information manually. Wrappers ensure that every `forward` and `backward` function automatically saves the required values in each module.\n",
    "\n",
    "**Example Usage**\n",
    "\n",
    "To apply a wrapper, we use a [**decorator**](https://www.geeksforgeeks.org/decorators-in-python/) notation (example below):\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "class MicroTorchLayer(Module):\n",
    "  @layer_forward_wrapper\n",
    "  def forward(self, x):\n",
    "    return some_operation_on_x\n",
    "\n",
    "  @layer_backward_wrapper\n",
    "  def backward(self, delta):\n",
    "    return delta * gradient_of_forward\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7t3329WYnebX"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY THIS CELL\n",
    "# Put this decorator on any forward function for layer modules\n",
    "def layer_forward_wrapper(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # The module is the first argument\n",
    "        module = args[0]\n",
    "        # The input is the second argument. Store it in the module\n",
    "        module._in = args[1]\n",
    "        result = fn(*args, **kwargs)\n",
    "        # Store the result of the forward function in the module\n",
    "        module._out = result\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Put this decorator on any backward function for layer modules\n",
    "def layer_backward_wrapper(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # The module is the first argument\n",
    "        module = args[0]\n",
    "        # The delta is the second argument. Store it in the module\n",
    "        module._delta = args[1]\n",
    "        result = fn(*args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Put this decorator on any forward function for layer modules\n",
    "def criterion_forward_wrapper(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # The module is the first argument\n",
    "        module = args[0]\n",
    "        # The input (y_hat) is the second argument. Store it in the module\n",
    "        module._in = args[1]\n",
    "        # The target (y) is the third argument. Store it in the module\n",
    "        module._target = args[2]\n",
    "        result = fn(*args, **kwargs)\n",
    "        # Store the result of the backward function in the module\n",
    "        module._out = result\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Put this decorator on any backward function for criterion modules\n",
    "def criterion_backward_wrapper(fn):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # The module is the first argument\n",
    "        module = args[0]\n",
    "        # The delta is the second argument. Store it in the module\n",
    "        module._delta = args[1]\n",
    "        result = fn(*args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70Pb7tVeb85I"
   },
   "source": [
    "## Criterion Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMZvxlPkYY0Z"
   },
   "source": [
    "Now let’s start by implementing our first MicroTorch modules. We’ll begin with two basic loss criterions: Mean Squared Error (MSE) and Cross-Entropy Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlPYKnedNZUr"
   },
   "source": [
    "### MSE Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4WSwjCfYY0Z"
   },
   "source": [
    "The **Mean Squared Error (MSE)** loss is one of the most common loss functions used for regression tasks. It calculates the average squared difference between predicted and actual values, making it useful for measuring the distance between predictions and targets.\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "In the forward pass, the MSE function computes the loss using:\n",
    "$$\n",
    "L(\\hat{y}, y) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "where:\n",
    "- $ \\hat{y}$ is the predicted value,\n",
    "- $ y$ is the target value,\n",
    "- $ n$ is the number of samples.\n",
    "\n",
    "**Note:** The input to MSE loss is batched, so you should return the average loss across the batch.\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "In the backward pass, the gradient of the loss with respect to the predictions is calculated as:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{2}{n} \\cdot (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "Let’s implement this in MicroTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wEcEqJIOzfZO"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "class MSE(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSE, self).__init__()\n",
    "\n",
    "    @criterion_forward_wrapper\n",
    "    def forward(self, x, y):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"MSE forward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    @criterion_backward_wrapper\n",
    "    def backward(self, delta=1.0):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"MSE backward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGecrPxBvKZi"
   },
   "source": [
    "### CrossEntropy Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wDSGkezYY0a"
   },
   "source": [
    "**Cross-Entropy** is a popular loss function for classification tasks. It measures the performance of a classification model whose output is a probability between 0 and 1. Cross-Entropy is especially useful when dealing with probabilities or logits.\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "The Cross-Entropy loss function is defined as:\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\sum_{i=1}^{C} y_i \\cdot \\log(\\hat{y}_i)\n",
    "$$\n",
    "where:\n",
    "- $ C$ is the number of classes,\n",
    "- $ y_i$ is the true label (one-hot encoded),\n",
    "- $ \\hat{y}_i$ is the predicted probability for class $i$.\n",
    "\n",
    "**Note:** The input to Cross-Entropy loss is batched, so you should return the average loss across the batch.\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "The gradient of the Cross-Entropy loss with respect to predictions is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{1}{C} \\cdot (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "**Note:** We divide the gradient of the Cross-Entropy loss by the number of classes since it ensures the gradient reflects the influence of each class equally. This is particularly important in multi-class classification tasks by preventing gradients from becoming too large when the number of classes increases.\n",
    "\n",
    "**Note:** You will want to convert your target labels in a One-Hot Encoded representation before calculating cross entropy with the model's outputs.\n",
    "\n",
    "Let’s implement this in MicroTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WEhR5NAsE0kB"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "class CrossEntropy(Criterion):\n",
    "  def __init__(self):\n",
    "    super(CrossEntropy, self).__init__()\n",
    "\n",
    "  @criterion_forward_wrapper\n",
    "  def forward(self, x, y):\n",
    "    ### YOUR CODE BELOW HERE ###\n",
    "    raise NotImplementedError(\"CrossEntropy forward function not implemented\")\n",
    "    ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "  @criterion_backward_wrapper\n",
    "  def backward(self, delta = 1.0):\n",
    "    ### YOUR CODE BELOW HERE ###x\n",
    "    raise NotImplementedError(\"CrossEntropy backward function not implemented\")\n",
    "    ### YOUR CODE ABOVE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i89mbg-lwrXz"
   },
   "source": [
    "## Layer Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDLqfPESYY0a"
   },
   "source": [
    "We will add several core layer types to our **MicroTorch** framework. Each layer will inherit from the base `Module` class we defined earlier. These layers include common activation functions and a linear layer, which are essential building blocks for neural networks.\n",
    "\n",
    "Specifically, we will implement:\n",
    "\n",
    "- **Sigmoid**\n",
    "\n",
    "- **ReLU**\n",
    "\n",
    "- **Linear**\n",
    "\n",
    "- **Softmax**\n",
    "\n",
    "Let’s start implementing these layers, starting with the activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gsK15xuNj-N"
   },
   "source": [
    "### Sigmoid Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ua2zE40MYY0a"
   },
   "source": [
    "\n",
    "The **Sigmoid** activation function maps input values between 0 and 1, making it useful for models that need to output probabilities.\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "The Sigmoid function is defined as:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "The gradient of the Sigmoid function is:\n",
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial x} = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "Let’s implement this as a MicroTorch layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Pgb-W8Mn0GeC"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    @layer_forward_wrapper\n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"Sigmoid forward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    @layer_backward_wrapper\n",
    "    def backward(self, delta=1.0):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"Sigmoid backward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otTg4cy-YY0a"
   },
   "source": [
    "### ReLU Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZh6RLjYYY0b"
   },
   "source": [
    "The **ReLU (Rectified Linear Unit)** activation function is commonly used in deep networks because it helps prevent vanishing gradients.\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "ReLU is defined as:\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "The gradient of ReLU is:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 1 \\text{ if } x > 0, \\text{ else } 0\n",
    "$$\n",
    "\n",
    "Let’s implement this as a MicroTorch Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p0u7MNqwYY0b"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    @layer_forward_wrapper\n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"ReLU forward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    @layer_backward_wrapper\n",
    "    def backward(self, delta=1.0):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"ReLU backward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY8uRJ2FYY0b"
   },
   "source": [
    "### Linear Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JW9-Oa5YY0b"
   },
   "source": [
    "The **Linear** layer (or fully connected layer) performs a linear transformation of the input data.\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "The Linear transformation is:\n",
    "$$\n",
    "f(x) = xW + b\n",
    "$$\n",
    "where:\n",
    "- $W$ is the weight matrix,\n",
    "- $b$ is the bias vector.\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "The gradient of the Linear layer with respect to the weights and inputs is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\delta \\cdot W^T\n",
    "$$\n",
    "\n",
    "Let’s implement this as a MicroTorch Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LYsWDwORfExj"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "class Linear(Module):\n",
    "    def __init__(self, input_size, output_size, bias=0.0):\n",
    "        super(Linear, self).__init__()\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"Linear __init__ function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    @layer_forward_wrapper\n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"Linear forward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    @layer_backward_wrapper\n",
    "    def backward(self, delta=1.0):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        # print(delta)\n",
    "        raise NotImplementedError(\"Linear backward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOuU1bgMsoxU"
   },
   "source": [
    "### Softmax Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNIxbKVJYY0c"
   },
   "source": [
    "The **Softmax** layer converts raw logits into a probability distribution, often used as the final layer in a classification network.\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "The Softmax function is:\n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "The Softmax layer assumes a gradient passthrough, meaning that the gradient is computed in conjunction with a loss function (like Cross-Entropy) that comes after the Softmax layer.\n",
    "\n",
    "Let’s implement this as a MicroTorch Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fhtpkjumMStQ"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "class Softmax(Module):\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "\n",
    "    @layer_forward_wrapper\n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"Softmax forward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    @layer_backward_wrapper\n",
    "    def backward(self, delta=1.0):\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr_Za_k7OM1L"
   },
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FRtq8ISYY0c"
   },
   "source": [
    "Finally, we bring all the components together in the `NeuralNetwork` class. This class allows us to create and train neural networks by chaining layers and applying the specified loss criterion.\n",
    "\n",
    "- **forward()**: Calls the `forward()` function of each layer in sequence, saving the intermediate results.\n",
    "- **compute_loss()**: Calculates the loss using the criterion function.\n",
    "- **backward()**: Performs backpropagation by chaining `backward()` calls for each layer in reverse order.\n",
    "- **update()**: Updates the model’s parameters using the computed gradients and the specified learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1hj2iPJabs2q"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, lr=0.01, layers=[], criterion=None):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        self.layers = layers  # Layers are Module objects\n",
    "        self.criterion = criterion  # criterion is a Criterion object\n",
    "        self.lr = lr  # Learning rate\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    # Run the forward pass of the neural network.\n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"NeuralNetwork forward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    # Compute the loss for the network, x is the prediction, y is the target\n",
    "    def compute_loss(self, x, y):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"NeuralNetwork compute_loss function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    # Run the backward pass of the neural network.\n",
    "    # This calls backward on all the layers, including the criterion and chains all the deltas.\n",
    "    def backward(self):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"NeuralNetwork backward function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    # Update the parameters of the neural network\n",
    "    def update(self):\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"NeuralNetwork update function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    # You can call a NeuralNetwork object like a function. This calls the forward function.\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    # Add a layer object\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Provide a list of layer objects\n",
    "    def set_layers(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    # Provide the criterion object\n",
    "    def set_criterion(self, criterion):\n",
    "        self.criterion = criterion\n",
    "\n",
    "    # Provide the learning rate\n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    # Clear the gradients of all the layers\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer._delta = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEFsGpg8YY0c"
   },
   "source": [
    "## XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyYfWY-6YY0c"
   },
   "source": [
    "Now that we have finished building our **MicroTorch** framework, it's time to put everything together and test it on a real problem: the XOR problem.\n",
    "\n",
    "The XOR (exclusive or) problem is a classic challenge in machine learning because it’s not linearly separable, meaning a single-layer perceptron cannot solve it. In XOR, the output is `1` when the inputs differ (e.g., `0, 1` or `1, 0`) and `0` when the inputs are the same (both `0` or both `1`).\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**XOR Truth Table**\n",
    "  \n",
    "| Input 1 | Input 2 | Output |\n",
    "|---------|---------|--------|\n",
    "|    0    |    0    |    0   |\n",
    "|    0    |    1    |    1   |\n",
    "|    1    |    0    |    1   |\n",
    "|    1    |    1    |    0   |\n",
    "\n",
    "</div>\n",
    "\n",
    "<!-- <p align=\"center\">\n",
    "  <img src=\"xor.png\" alt=\"XOR Problem Explanation\" height=\"400\">\n",
    "</p> -->\n",
    "\n",
    "Because XOR requires non-linear boundaries, a neural network with at least one hidden layer and a non-linear activation function is necessary. This configuration allows the network to learn complex patterns and transform the XOR data into a linearly separable form in the hidden layer, allowing for accurate classification.\n",
    "\n",
    "We will use the XOR problem as a simple test case to verify if our MicroTorch implementation is working correctly. This problem also demonstrates the power of even very small neural networks, as they can solve tasks that are not possible with simpler linear models.\n",
    "\n",
    "For a more detailed explanation of the XOR problem and why it’s challenging for linear models, check out this [great explanation](https://wangkuiyi.github.io/xor.html).\n",
    "\n",
    "We will tackle the XOR problem in two ways:\n",
    "- **As a Classification Task**: We treat XOR as a binary classification problem, where the output is either 0 or 1.\n",
    "- **As a Regression Task**: Here, we will predict continuous values near 0 and 1, making the task slightly more challenging.\n",
    "\n",
    "By comparing these approaches, we’ll see how neural networks can be adapted for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MCe8CWzYYY0c"
   },
   "outputs": [],
   "source": [
    "input_csv = \"\"\"\n",
    "observation,input1,input2,output\n",
    "1,0,0,0\n",
    "2,0,1,1\n",
    "3,1,0,1\n",
    "4,1,1,0\n",
    "\"\"\"\n",
    "xor_dataset = pd.read_csv(io.StringIO(input_csv), index_col=\"observation\")\n",
    "xor_inputs = xor_dataset.iloc[:,:-1].to_numpy().astype('float32')\n",
    "xor_targets = xor_dataset.iloc[:,-1].to_numpy().reshape(-1, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfazBYGBQSVX"
   },
   "source": [
    "## Building a Neural Network using MicroTorch to Solve XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko72QVrFYY0d"
   },
   "source": [
    "Let's walk through the steps required to set up and train a neural network in MicroTorch. Specifically, we’ll define the model architecture, choose a loss criterion, and set up the training process to observe how our model learns over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xYp3ejzVh5Y"
   },
   "source": [
    "**Define the Model**: Create an instance of `NeuralNetwork`, specifying the learning rate and the structure:\n",
    "- **Add Layers**: Set up the layers to create a model architecture, either by providing a list of modules or by adding them one at a time.\n",
    "- **Specify Loss Criterion**: Choose an appropriate loss criterion.\n",
    "\n",
    "**Training Loop Implementation**:\n",
    "- **Forward Pass**: Pass the input data through the model to get predictions.\n",
    "- **Compute Loss**: Compare predictions to actual targets to calculate the loss.\n",
    "- **Backward Pass**: Use backpropagation to calculate gradients for each layer.\n",
    "- **Update Parameters**: Adjust each layer’s parameters based on the gradients to minimize the loss.\n",
    "\n",
    "We repeat these steps until the model’s performance stabilizes or the loss reaches a satisfactory level. Let’s start training!\n",
    "\n",
    "**Hyperparameter Tuning**:\n",
    "If training doesn’t converge, try adjusting `NUM_ITERATIONS` and `LR`:\n",
    "- `LR`: If the loss fluctuates, reduce `LR`. If it’s decreasing too slowly, increase LR.\n",
    "- `NUM_ITERATIONS`: Increase if the model seems to be learning but hasn’t reached a satisfactory loss.\n",
    "\n",
    "Tweaking these parameters should help achieve convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6GGZtIoYY0d"
   },
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQimpE0OYY0d"
   },
   "source": [
    "\n",
    "For the classification model, we’ll design a neural network that treats XOR as a binary classification problem. The network should include:\n",
    "- A **final layer** that outputs class probabilities.\n",
    "- **Cross-entropy** as the loss function to measure performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bCH8aSu7YY0d"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "# Initialize the layers, criterion, and model with classification objective\n",
    "def setup_classification_model():\n",
    "    \"\"\"\n",
    "    Write your layers as a list of Layer modules (you have flexibility as to the architecture,\n",
    "    though we recommend using a couple linear layers). Set the criterion as CrossEntropy\n",
    "    \"\"\"\n",
    "\n",
    "    ce_model = None\n",
    "    ### YOUR CODE BELOW HERE ###\n",
    "    raise NotImplementedError(\"setup_classification_model function not implemented\")\n",
    "    ### YOUR CODE ABOVE HERE ###\n",
    "    return ce_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SIuRIwWD07Sw"
   },
   "outputs": [],
   "source": [
    "ce_model = setup_classification_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0anvh-DyYY0d"
   },
   "source": [
    "### Regression Model\n",
    "\n",
    "For the regression model, we’ll design a neural network that learns to approximate XOR as a continuous output problem. The network should be configured to output values close to 0 and 1 for each input pair, rather than binary class labels.\n",
    "- Use a **mean squared error (MSE)** loss function to measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-xbwZZKLYY0d"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "# Initialize the layers, criterion, and model with regression objective\n",
    "def setup_regression_model():\n",
    "    \"\"\"\n",
    "    Write your layers as a list of Layer modules (you have flexibility as to the architecture,\n",
    "    though we recommend using a couple linear layers). Set the criterion as MSE.\n",
    "    \"\"\"\n",
    "\n",
    "    mse_model = None\n",
    "\n",
    "    ### YOUR CODE BELOW HERE ###\n",
    "    raise NotImplementedError(\"setup_regression_model function not implemented\")\n",
    "    ### YOUR CODE ABOVE HERE ###\n",
    "    return mse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "aagMKTpW0rN5"
   },
   "outputs": [],
   "source": [
    "mse_model = setup_regression_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-oQHELDYY0d"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FEh7gdj2vWkv"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "def train_model(model, x, y, mode='classification', lr=0.1, num_iterations=50000, plot_losses=True):\n",
    "    \"\"\"\n",
    "    Train a neural network on the XOR problem with a specified mode (classification or regression).\n",
    "\n",
    "    Parameters:\n",
    "    - model (NeuralNetwork): The neural network model to train.\n",
    "    - mode (str): The mode of the task, either 'classification' or 'regression'.\n",
    "    - lr (float): Learning rate.\n",
    "    - num_iterations (int): Number of training iterations.\n",
    "    - plot_losses (bool): Whether to plot the losses.\n",
    "    \"\"\"\n",
    "    y = y.astype('int32') if mode == 'classification' else y\n",
    "    # Training loop\n",
    "    loss_values = []\n",
    "    model.zero_grad()  # Reset gradients\n",
    "    model.set_lr(lr)  # Set learning rate\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        \"\"\"\n",
    "        Write code to do the forward, loss, backwards, and update routine\n",
    "        Please store your loss in a variable called `loss`\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"train_model function not implemented\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "        # Store and print loss\n",
    "        loss_values.append(loss)\n",
    "        if plot_losses == True and i % 2000 == 0:\n",
    "            print(f\"Iteration {i:5d} | Loss: {loss:.6f}\")\n",
    "            plot_loss(loss_values, dynamic=True)\n",
    "\n",
    "    # Final plot\n",
    "    if plot_losses == True:\n",
    "        plot_loss(loss_values, dynamic=False)\n",
    "        plt.show()\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81J9oRxFYY0d"
   },
   "source": [
    "### Train XOR as a Classification Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st_IR4kIYY0d"
   },
   "source": [
    "Now we're ready to start training our the XOR model as a classification task.\n",
    "After training, we will check the model’s predictions and calculate the classification accuracy.\n",
    "\n",
    "**Notes**: \n",
    "- Your learning rate (`lr`) must be a positive float.\n",
    "- The number of iterations (`num_iterations`) you run must be a positive integer that is a multiple of 10, and must also be less than 50,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ky7DbAyf2Oro"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "# Set classification Hyperparameters for training here - we've given default values - these may not be optimal,\n",
    "# so you will want to experiment with different values\n",
    "def set_classification_parameters():\n",
    "    # YOUR CODE BELOW HERE\n",
    "    LR = 0.5\n",
    "    NUM_ITERATIONS = 1\n",
    "    # YOUR CODE ABOVE HERE\n",
    "    return LR, NUM_ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MLHiaGVd_l4k"
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "x = xor_inputs\n",
    "y = xor_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1680,
     "status": "ok",
     "timestamp": 1733159393144,
     "user": {
      "displayName": "Kiran Nazarali",
      "userId": "09104139499711596376"
     },
     "user_tz": 300
    },
    "id": "gBWURWGSYY0d",
    "outputId": "14821f95-cb22-4c27-cf78-e329c1e4957d"
   },
   "outputs": [],
   "source": [
    "LR, NUM_ITERATIONS = set_classification_parameters()\n",
    "\n",
    "# For classification\n",
    "_ = train_model(ce_model, x, y, mode='classification', lr=LR, num_iterations=NUM_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBCpViFeYY0e"
   },
   "outputs": [],
   "source": [
    "# Calculate the Classification Accuracy\n",
    "y_hat = ce_model(xor_inputs.astype('int'))\n",
    "y_pred = np.argmax(y_hat, axis=1)\n",
    "accuracy = np.mean(y_pred == xor_targets.squeeze())\n",
    "\n",
    "print(\"\\nModel Predictions:\")\n",
    "print(y_pred.reshape(-1, 1))\n",
    "\n",
    "print(\"\\nTarget Values:\")\n",
    "print(xor_targets.astype('int'))\n",
    "\n",
    "print(f\"\\nClassification Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoZsly2TYY0e"
   },
   "source": [
    "### Train XOR as Regression Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkcbN_bjYY0e"
   },
   "source": [
    "Next, we’ll train the XOR model as a regression task.\n",
    "After training, we will measure the model’s performance using the **Mean Absolute Difference** between predictions and targets.\n",
    "\n",
    "**Notes**: \n",
    "- Your learning rate (`lr`) must be a positive float.\n",
    "- The number of iterations (`num_iterations`) you run must be a positive integer that is a multiple of 10, and must also be less than 50,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "HU0Hp2R72VjL"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "# Set regression hyperparameters for training here - we've given default values - these may not be optimal,\n",
    "# so you will want to experiment with different values\n",
    "def set_regression_parameters():\n",
    "    # YOUR CODE BELOW HERE\n",
    "    LR = 0.5\n",
    "    NUM_ITERATIONS = 1\n",
    "    # YOUR CODE ABOVE HERE\n",
    "    return LR, NUM_ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svMWTSugYY0e"
   },
   "outputs": [],
   "source": [
    "LR, NUM_ITERATIONS = set_regression_parameters()\n",
    "\n",
    "# For regression\n",
    "_ = train_model(mse_model, x, y, mode='regression', lr=LR, num_iterations=NUM_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2g6pm7UYY0e"
   },
   "outputs": [],
   "source": [
    "y_hat = mse_model(xor_inputs)\n",
    "y = xor_targets\n",
    "\n",
    "print(\"Model Outputs:\")\n",
    "print(np.round(y_hat, decimals=3))\n",
    "\n",
    "print(\"\\nTarget Values:\")\n",
    "print(y)\n",
    "\n",
    "mean_difference = np.mean(np.abs(y - y_hat))\n",
    "print(f\"\\nMean Absolute Difference (Error): {mean_difference:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xrspi1xOyWcB"
   },
   "source": [
    "# Grading\n",
    "\n",
    "You score for this part of the assignment will be out of **50 points**. A more detailed breakdown is as follows: \n",
    "- 35 points will be towards implementing all the MicroTorch modules correctly. \n",
    "- 15 points will be towards implementing the XOR network in MicroTorch correctly (100% classification accuracy, low MAE). \n",
    "\n",
    "There will be sanity checks that will give you a rough idea of how well you are doing. They will be independent test cases that will follow the same score scaling - make sure to check the outputs of those test cases in Gradescope! Performance on these sanity checks is a good indication, however, these tests are not exhaustive and do not guarantee any final score. We'll test your network and module functions on a variety of inputs and seeds. You should test your code thoroughly (feel free to write your own tests, and verify your implementations against PyTorch or TensorFlow). \n",
    "\n",
    "For the hidden tests for the XOR problem, we will run your code on various seeds to ensure its consistency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrRHYKs_yZIU"
   },
   "source": [
    "# Submission\n",
    "Upload this notebook with the name `submission.ipynb` file to Gradescope. The autograder will **only** run successfully if your file is named this way. You must ensure that you have removed all print statements from **your** code, or the autograder may fail to run. Excessive print statements will also result in muddled test case outputs, which makes it more difficult to interpret your score.\n",
    "\n",
    "We've added appropriate comments to the top of certain cells for the autograder to export (`# export`). You do NOT have to do anything (e.g. remove print statements) to cells we have provided - anything related to those have been handled for you. You are responsible for ensuring your own code has no syntax errors or unnecessary print statements. You ***CANNOT*** modify the export comments at the top of the cells, or the autograder will fail to run on your submission.\n",
    "\n",
    "You should ***not*** add any cells that your code requires to the notebook when submitting. You're welcome to add any code as you need to extra cells when testing, but they will not be graded. Only the provided cells will be graded. As mentioned in the top of the notebook, **any helper functions that you add should be nested within the function that uses them.**\n",
    "\n",
    "If you encounter any issues with the autograder, please feel free to make a post on Ed Discussion. We highly recommend making a public post to clarify any questions, as it's likely that other students have the same questions as you! If you have a question that needs to be private, please make a private post.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CS-3600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
