{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT6Ci5F2YY0U"
   },
   "source": [
    "# Introduction to Neural Networks - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are a core component of deep learning, driving advancements in image recognition, natural language processing, robotics, and more. These networks are made up of interconnected layers that transform data through a series of operations, allowing them to learn complex patterns. At the heart of neural networks is the process of learning through gradients, calculated by *backpropagation* and the *chain rule*, which enables the network to adjust its parameters based on how much each parameter contributes to the overall error.\n",
    "\n",
    "Modern deep learning frameworks like **PyTorch** and **TensorFlow** provide powerful abstractions for building and training neural networks, automatically handling gradient calculations and optimization steps. In this part of the assignment, we'll be using PyTorch!\n",
    "\n",
    "In this part of the assignment, we will build a mini neural network framework that supports gradient propagation, allowing us to train a network on a real task and observe how it learns. Specifically, you will:\n",
    "\n",
    "1. **Learn how to handle data inputs**: You will first implement some helper functions that will help you properly process input data and labels for training and evaluation. \n",
    "3. **Implement a Neural Network for Digit Classification (MNIST) using PyTorch**: After building a neural network to solve the XOR problem from scratch, you’ll implement another neural network in PyTorch to experience firsthand how these frameworks abstract many of the complex tasks involved in gradient calculations to solve a classic problem: the MNIST digit classification task. You will also get to tinker with your trained model to see how it performs on some drawings that you'll make!\n",
    "3. **Train and Evaluate on the [Rig Juice](https://regularshow.fandom.com/wiki/Rig_Juice) Classification Task**: Finally, you will train both your custom implementation and the PyTorch version on a *Rig juice classification task*, exploring how each approach learns to classify samples of Rig juice based on a variety of features. This task will help you understand the practical differences between manual implementation and using a modern framework.\n",
    "\n",
    "**Notes:**\n",
    "- We've provided all the imports that you'll need for this assignment, though you're welcome to add imports _from the Python Standard Library only_ if you need them. If you want to add any helper functions for whatever reason, please make sure that they are nested inside the function that they are used in. This is to ensure that the autograder can run your code without any issues.\n",
    "- ***DO NOT REMOVE ANY COMMENTS THAT HAVE `# export` IN THEM. THE GRADING SCRIPT USES THESE COMMENTS TO EVALUATE YOUR FUNCTIONS. WE WILL NOT AUDIT SUBMISSIONS TO ADD THESE. IF THE AUTOGRADER FAILS TO RUN DUE TO YOUR MODIFICATION OF THESE COMMENTS, YOU WILL NOT RECEIVE CREDIT.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72_ombXDbd5g"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z62QX-pqunOY"
   },
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY THIS CELL\n",
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import plot_loss\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "# add any additional imports here (from the Python Standard Library only!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_E-tgGnYY0e"
   },
   "source": [
    "# Neural Networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTFZnTrOYY0e"
   },
   "source": [
    "Now that you have a solid foundation for understanding how the ins and outs of a neural network functions, in this section, we’ll use PyTorch (an industry-standard library) to create a neural network. PyTorch provides predefined layers, optimizers, and utilities that make network training more efficient and flexible.\n",
    "\n",
    "This part of the assignment will allow you to explore the difference in setup and convenience between implementing a network from scratch and using a framework like PyTorch. Specifically, we'll be using PyTorch to implement neural networks to solve two different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our neural networks for either problem, we first have to prepare the data by shuffling it and splitting it into training and testing sets. \n",
    "\n",
    "The training set is used to train the model, allowing it to learn the underlying patterns and relationships within the data. In contrast, the testing set serves as a separate dataset that the model has not seen during training. This separation is critical for evaluating the model's performance and its ability to generalize. \n",
    "\n",
    "By using a testing set, we can assess how well the model can make predictions on new, unseen data, which is a key indicator of its effectiveness in real-world applications. Additionally, shuffling the data helps to ensure that the training and testing sets are representative of the overall dataset, reducing the risk of bias and improving the model.\n",
    "\n",
    "Implement the below helper functions to load and prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling ensures that the data is mixed randomly, which helps the model learn more effectively. \n",
    "\n",
    "Implement the below function to shuffle the data. Hint: You may find [torch.randperm](https://pytorch.org/docs/stable/generated/torch.randperm.html) useful for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "def shuffle_data(features: torch.Tensor, labels: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Shuffle the features and labels of a dataset.\n",
    "    Returns a tuple of shuffled features and shuffled labels.\n",
    "    '''\n",
    "    shuffled_features, shuffled_labels = None, None\n",
    "\n",
    "    # YOUR CODE BELOW HERE\n",
    "    raise NotImplementedError(\"shuffle_data is not implemented.\")\n",
    "    # YOUR CODE ABOVE HERE\n",
    "\n",
    "    return shuffled_features, shuffled_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is shuffled, we can split it into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance. The split ratio is typically 80% for training and 20% for testing. Though this is something you can tune, we've set this ratio for you to keep things simple. \n",
    "\n",
    "Implement the below helper function to split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "def split_data(features: torch.Tensor, labels: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Splits the data into a train and test set using\n",
    "    the defined constant, TRAIN_TEST_SPLIT_RATIO. Please\n",
    "    note that you *cannot* modify this constant.\n",
    "    '''\n",
    "    TRAIN_TEST_SPLIT_RATIO = 0.8\n",
    "    train_features, train_labels = None, None\n",
    "    test_features, test_labels = None, None\n",
    "\n",
    "    # YOUR CODE BELOW HERE\n",
    "    raise NotImplementedError(\"split_data is not implemented.\")\n",
    "    # YOUR CODE ABOVE HERE\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has two very useful abstractions, Dataset and DataLoader, to simplify how we handle data during training.\n",
    "- `Dataset`: An abstract class representing a dataset. Custom datasets inherit from this class and must implement __len__() and __getitem__(). For common use cases like tensors, PyTorch offers ready-made implementations such as `TensorDataset`, which is what we'll use. This class allows you to store inputs and their corresponding labels as tensors, making them easy to access and manipulate.\n",
    "- `DataLoader`: Combines a dataset with an iterable. It handles batching, shuffling, and parallel loading with minimal code. This is _especially_ useful for feeding data into a model efficiently during training and evaluation.\n",
    "\n",
    "Your task is to implement the below function to create a data loader using the PyTorch helper functions. We'll eventually use this data loader to feed data into our neural networks during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "def create_dataloader(train_features: torch.Tensor, train_labels: torch.Tensor, batch_size: int = 64) -> DataLoader:\n",
    "    \"\"\"Creates a DataLoader for the training features and labels.\"\"\"\n",
    "    train_loader = None\n",
    "\n",
    "    # YOUR CODE BELOW HERE\n",
    "    raise NotImplementedError(\"create_dataloader is not implemented.\")\n",
    "    # YOUR CODE ABOVE HERE\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've written helper methods to prepare the data, let's move onto the first problem: **MNIST Digit Classification**. In this problem, we will build a neural network to classify handwritten digits from the MNIST dataset. The MNIST dataset is a well-known benchmark in the field of machine learning and computer vision, consisting of 70,000 grayscale images of handwritten digits (0-9) and their corresponding labels. Each image is 28x28 pixels in size.\n",
    "\n",
    "We’ll load in the data and convert it into [PyTorch tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html), which are the primary data structures used in PyTorch for building and training neural networks.\n",
    "\n",
    "Before diving into training, let's visualize some examples from the dataset to better understand the kind of data we'll be working with. Run the below cell to see some of the images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "sample_indices = np.random.randint(0, len(mnist_dataset), size=5)\n",
    "fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "fig.suptitle('Sample MNIST Images')\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    image, label = mnist_dataset[idx]\n",
    "    axs[i].imshow(image.squeeze(), cmap='gray')\n",
    "    axs[i].set_title(f'Label: {label}')\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "images = torch.stack([img for img, _ in mnist_dataset])\n",
    "labels = torch.tensor([label for _, label in mnist_dataset])\n",
    "\n",
    "print(f\"Number of Samples: {len(images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Your Neural Network to Classify Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our own neural network for the MNIST problem using PyTorch. For this problem, your network can have any number of layers and nodes, as long as it is set up for **classification** (not regression). \n",
    "\n",
    "Here are some core PyTorch components we recommend you use to build and train your model (click the hyper links to learn more about each component):\n",
    "\n",
    "1. [**`nn.Module`**](https://pytorch.org/docs/stable/generated/torch.nn.Module.html): The base class for all neural network modules in PyTorch. By subclassing `nn.Module`, you can define and structure your own neural networks. Each layer of the network (like fully connected layers or activation functions) will be an attribute of this module, and the `forward()` method will define the data flow through these layers.\n",
    "\n",
    "2. [**`nn.Linear`**](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): This layer applies a linear transformation to the incoming data, making it useful for fully connected (dense) layers in a neural network.\n",
    "\n",
    "3. [**`nn.Sigmoid`**](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html): A common activation function that maps input values to an output range of `[0, 1]`.\n",
    "\n",
    "4. [**`nn.CrossEntropyLoss`**](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html): This is a loss function suited for classification tasks, especially where classes are mutually exclusive. `nn.CrossEntropyLoss` combines `log_softmax` and `nll_loss` in a single function, which makes it numerically more stable.\n",
    "\n",
    "PyTorch offers a wide range of layers, activation functions, and loss functions for building neural networks, making it a flexible and powerful tool for experimentation. **While the components mentioned above are enough to complete this task, we encourage you to explore more.** There are many other layers, loss functions, and activation functions (some you've learned about, some new!) that may help you solve different types of problems or improve model performance.\n",
    "\n",
    "Check out the full list of [PyTorch's Neural Network Layers and Loss Functions](https://pytorch.org/docs/stable/nn.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "# Initialize the layers, criterion, and model with classification objective\n",
    "class MNISTNeuralNetwork(nn.Module):\n",
    "    # Define the layers of the model that will be used in the forward pass\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "        super(MNISTNeuralNetwork, self).__init__()\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"MNISTNeuralNetwork is not implemented.\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    # Construct the forward pass using the defined layers and pass the input x through each layer\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"MNISTNeuralNetwork forward pass is not implemented.\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following hyperparameters:\n",
    "- `EPOCHS`: Number of times the entire training dataset is passed through the model. This should be a positive integer that is a multiple of 10. \n",
    "- `LR` (Learning Rate): Controls how much to adjust the model weights during training. This should be a positive float. \n",
    "- `BATCH_SIZE`: Number of samples processed before the model is updated. This should be a positive integer that that is a power of 2. \n",
    "- `EVAL_EVERY`: Frequency (in epochs) at which to evaluate the model on the validation set. This can be any positive integer. \n",
    "    \n",
    "Feel free to experiment with different values to see how they affect training. We've put in placeholders as default values which might not be the best for your model, so feel free to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "def set_mnist_hyperparameters() -> None:\n",
    "    '''\n",
    "    Sets the hyperparameters for the MNIST model.\n",
    "    '''\n",
    "    global EPOCHS, LR, BATCH_SIZE, EVAL_EVERY\n",
    "    \n",
    "    ### YOUR CODE BELOW HERE ###\n",
    "    EPOCHS = 1\n",
    "    LR = 0.5\n",
    "    BATCH_SIZE = 32\n",
    "    EVAL_EVERY = 20\n",
    "    ### YOUR CODE ABOVE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mnist_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST PyTorch Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our PyTorch model, let’s set up a training loop to optimize the model parameters. This loop will look very similar to the MicroTorch training loop but with PyTorch-specific syntax.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Optimizer**: PyTorch provides a variety of optimizers for updating model parameters based on computed gradients. Here, we will use `optim.SGD` for stochastic gradient descent. The optimizer is initialized with the model parameters and learning rate.\n",
    "   - [PyTorch Documentation for `optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)\n",
    "\n",
    "2. **`.backward()`**: This method computes the gradient of the loss with respect to each parameter using backpropagation. PyTorch handles all gradient calculations automatically, so you don’t need to manually derive them as in MicroTorch.\n",
    "\n",
    "3. **`.zero_grad()`**: Before each backward pass, we need to reset the gradients for each parameter by calling `optimizer.zero_grad()`. This ensures that gradients from previous steps don’t accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "def train_model_pytorch_mnist(images, labels, plot_losses=True):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "\n",
    "    # Initialize model, loss, optimizer, and the set the hyperparameters\n",
    "    ### YOUR CODE BELOW HERE ###\n",
    "    raise NotImplementedError(\"train_model_pytorch_mnist setup is not implemented.\")\n",
    "    ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    # Create DataLoaders\n",
    "    images, labels = shuffle_data(images, labels)\n",
    "    train_features, train_labels, test_features, test_labels = split_data(\n",
    "        images, labels\n",
    "    )\n",
    "    train_loader = create_dataloader(train_features, train_labels, batch_size=BATCH_SIZE)\n",
    "    test_loader = create_dataloader(test_features, test_labels, batch_size=BATCH_SIZE)\n",
    "\n",
    "    for epoch in range(EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \"\"\"\n",
    "        Implement the training loop steps:\n",
    "        1. Forward pass through the model\n",
    "        2. Calculate loss\n",
    "        3. Zero gradients\n",
    "        4. Backward pass\n",
    "        5. Update parameters\n",
    "        \"\"\"\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            ### YOUR CODE BELOW HERE ###\n",
    "            raise NotImplementedError(\"train_model_pytorch_mnist training loop is not implemented.\")\n",
    "            ### YOUR CODE ABOVE HERE ###\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluate on validation set every EVAL_EVERY epochs\n",
    "        if epoch % EVAL_EVERY == 0:\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            total_correct = 0\n",
    "\n",
    "            \"\"\"\n",
    "            Implement the evaluation loop steps:\n",
    "            1. Forward pass through the model (no need for gradients)\n",
    "            2. Calculate loss\n",
    "            3. Get predictions and compare with true labels\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                for batch_features, batch_labels in test_loader:\n",
    "                    ### YOUR CODE BELOW HERE ###\n",
    "                    raise NotImplementedError(\"train_model_pytorch_mnist evaluation loop is not implemented.\")\n",
    "                    ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(test_loader)\n",
    "            accuracy = (total_correct / len(test_loader.dataset)) * 100\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "                f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                f\"Validation Loss: {avg_val_loss:.4f} | \"\n",
    "                f\"Accuracy: {accuracy:.2f}%\"\n",
    "            )\n",
    "\n",
    "            if plot_losses:\n",
    "                plot_loss(\n",
    "                    train_losses, val_losses, eval_interval=EVAL_EVERY, dynamic=True\n",
    "                )\n",
    "\n",
    "    print(\n",
    "        f\"Final Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Final Validation Loss: {avg_val_loss:.4f} | \"\n",
    "        f\"Final Accuracy: {accuracy:.2f}%\"\n",
    "    )\n",
    "    if plot_losses:\n",
    "        plot_loss(train_losses, val_losses, eval_interval=EVAL_EVERY, dynamic=False)\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model, train_losses, val_losses = train_model_pytorch_mnist(images, labels, plot_losses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should achieve an accuracy of above 95% in this portion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Number of test samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "total_correct = 0\n",
    "mnist_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_samples, test_labels in test_loader:\n",
    "        test_outputs = mnist_model(test_samples)\n",
    "        predicted_labels = torch.argmax(test_outputs, dim=1)\n",
    "        total_correct += (predicted_labels == test_labels).sum().item()\n",
    "\n",
    "accuracy = (total_correct / len(test_loader.dataset)) * 100\n",
    "print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Drawing some MNIST Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to test your code further, we encourage you to draw some digits using the below cell. It will create a sketchpad for you to draw digits, and then it will use your trained model to predict the digit you drew.\n",
    "\n",
    "Note that this will not be graded and is purely for your own exploration. Results may vary, as the images that you are creating are not exactly the same as the MNIST dataset. However, it should be a fun way to see how your model performs on new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def predict_digit(image_dict: dict) -> str:\n",
    "    '''\n",
    "    Converts the input image to a tensor and predicts the digit.\n",
    "    Returns the predicted digit as a string.\n",
    "    '''\n",
    "    image = image_dict[\"composite\"]\n",
    "        \n",
    "    # Uncomment the below line to see the raw input image for debugging\n",
    "    # Image.fromarray(image).save(\"raw_input.png\")\n",
    "    \n",
    "    # RGBA images\n",
    "    if image.shape[2] == 4:\n",
    "        # Extract the alpha channel\n",
    "        alpha = image[..., 3]\n",
    "        \n",
    "        # Create a mask where alpha > 0 (something's drawn here)\n",
    "        mask = alpha > 0\n",
    "        \n",
    "        # For MNIST, we want black background (0) and white digits (255)\n",
    "        result = np.zeros_like(alpha) # Black background\n",
    "        result[mask] = 255 # Set drawn areas to white (255)\n",
    "    else:\n",
    "        # If RGB only, convert to grayscale\n",
    "        gray = np.mean(image, axis=2).astype(np.uint8)\n",
    "        result = gray\n",
    "        \n",
    "    # Convert to PIL and save/debug\n",
    "    image_pil = Image.fromarray(result)\n",
    "\n",
    "    # Uncomment the below line to see the processed image for debugging\n",
    "    # image_pil.save(\"digit_debug.png\")\n",
    "    \n",
    "    # Apply transforms\n",
    "    digit = transform(image_pil).unsqueeze(0)\n",
    "    \n",
    "    # Save the tensor image for debugging\n",
    "    transformed_image = digit.squeeze().numpy()\n",
    "    \n",
    "    # Properly reverse the normalization for visualization\n",
    "    vis_img = ((transformed_image * 0.5 + 0.5) * 255).astype(np.uint8)\n",
    "\n",
    "    # Uncomment the below line to see the input image for the model for debugging\n",
    "    Image.fromarray(vis_img).save(\"model_input.png\")\n",
    "\n",
    "    mnist_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = mnist_model(digit)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    \n",
    "    return f\"Predicted Digit: {pred}\"\n",
    "\n",
    "app = gr.Interface(fn=predict_digit, inputs=\"sketchpad\", outputs=\"text\", live=False)\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to stop the app connection to the server in the notebook\n",
    "app.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rig Juice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When wandering the vast expanse of TextWorld, you've encountered a special power-up, courtesy of your good friend, Rigby: **RIG JUICE**. It's a strange, vibrant liquid that seems to have a mind of its own. It can be used to power up your search through TextWorld, but it also has a tendency to cause the agent to explode if it's unsafe. Luckily, you've been given a dataset with the characteristics of the different possible Rig Juice flavors (Rigby tends to invent new flavors on the fly). \n",
    "\n",
    "\n",
    "Rig juice is a juice that contains random items that Rigby finds, and since its conception, it has become a highly-valued substance in the TextWorld economy. Although little is known about the makeup of each jug of Rig juice found, they have unique ingredients and have been made with mysterious processes that make them highly sought after by explorers. For more information about Rig juice and its wondrous effects, [check out the following link](https://regularshow.fandom.com/wiki/Rig_Juice). \n",
    "\n",
    "Because they can be made with some wild ingredients, we need to make sure that the Rig juice that we're dealing with is safe. Many health experts in the TextWorld community have found poisonous jugs of Rig juice in the wild (Rigby's a careless guy sometimes). Our task is to use a neural network to determine whether a given jug of Rig juice is **poisonous** or **non-poisonous**, to ensure that our agent doesn't get injured in their journey. By training our neural network on this dataset, we want to achieve a high accuracy in a [binary classification](https://www.learndatasci.com/glossary/binary-classification/) of Rig juice. This will help improve our understanding the uniqueness of Rig juice to harness its utility to effectively search through TextWorld.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Rig Juice Dataset and Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the dataset and prepare the features and labels for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"rig_juice_student.csv\")\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the dataset columns (features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset columns:\")\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains various features (attributes; in this case, ingredients) of a sample of Rig Juice and a label indicating their class. The first column represents the label (class) which'll tell us whether something is poisonous or not, and the remaining columns are the features.\n",
    "\n",
    "As we did before, we'll convert the data to [PyTorch tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) for ease of use in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to tensors\n",
    "labels = torch.tensor(dataset[\"poisonous\"].to_numpy(), dtype=torch.long)\n",
    "features = torch.tensor(dataset.drop(columns = [\"poisonous\"]).to_numpy(), dtype=torch.float32)\n",
    "\n",
    "print(f\"Number of samples: {len(features)}\")\n",
    "print(f\"Number of features: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define the neural network model for our binary classification task of the Rig juice dataset to determine whether samples of the Rig juice are poisonous or not. We'll use PyTorch's `nn.Module` class to create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "class RigJuiceNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "        super(RigJuiceNeuralNetwork, self).__init__()\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"RigJuiceNeuralNetwork is not implemented.\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ### YOUR CODE BELOW HERE ###\n",
    "        raise NotImplementedError(\"RigJuiceNeuralNetwork forward pass is not implemented.\")\n",
    "        ### YOUR CODE ABOVE HERE ###\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rig Juice Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following hyperparameters:\n",
    "- `EPOCHS`: Number of times the entire training dataset is passed through the model. This should be a positive integer that is a multiple of 10. \n",
    "- `LR` (Learning Rate): Controls how much to adjust the model weights during training. This should be a positive float. \n",
    "- `BATCH_SIZE`: Number of samples processed before the model is updated. This should be a positive integer that that is a power of 2. \n",
    "- `EVAL_EVERY`: Frequency (in epochs) at which to evaluate the model on the validation set. This can be any positive integer. \n",
    "    \n",
    "Feel free to experiment with different values to see how they affect training. We've put in placeholders as default values which might not be the best for your model, so feel free to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "def set_rig_juice_hyperparameters() -> None:\n",
    "    '''\n",
    "    Sets the hyperparameters for the MNIST model.\n",
    "    '''\n",
    "    global EPOCHS, LR, BATCH_SIZE, EVAL_EVERY\n",
    "    \n",
    "    ### YOUR CODE BELOW HERE ###\n",
    "    EPOCHS = 1\n",
    "    LR = 0.5\n",
    "    BATCH_SIZE = 32\n",
    "    EVAL_EVERY = 20\n",
    "    ### YOUR CODE ABOVE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rig_juice_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rig Juice Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the missing code to complete the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export - DO NOT MODIFY OR MOVE THIS LINE\n",
    "def train_model_pytorch_rig_juice(features, labels, plot_losses=True):\n",
    "    '''\n",
    "    In this function, you will implement the training loop for the Rig Juice dataset.\n",
    "    The training loop will include the following steps:\n",
    "    - Initialize the model, loss function, and optimizer\n",
    "    - Shuffle the data, split it into training and test sets\n",
    "    - train the model using the training set \n",
    "        - for each batch in each epoch, perform the forward pass, calculate loss, zero gradients, and backward pass\n",
    "    - Evaluate the model using the test set \n",
    "    - Plot the training and validation losses (if you wish)\n",
    "    - Return the model, criterion, optimizer, training losses, validation losses, and test set\n",
    "    '''\n",
    "    # Initialize lists to store loss values\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Initialize the model, loss, and optimizer, and set the hyperparameters\n",
    "    # please name these respectively as `model`, `criterion`, and `optimizer`\n",
    "    # YOUR CODE BELOW HERE\n",
    "    raise NotImplementedError(\"train_model_pytorch_rig_juice model initialization is not implemented.\")\n",
    "    # YOUR CODE ABOVE HERE\n",
    "\n",
    "    # preparing the data\n",
    "    features, labels = shuffle_data(features, labels)\n",
    "    train_features, train_labels, test_features, test_labels = split_data(features, labels)\n",
    "    train_loader = create_dataloader(train_features, train_labels, batch_size=BATCH_SIZE)\n",
    "\n",
    "    for i in range(EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            \"\"\"\n",
    "            Implement the training loop steps:\n",
    "            1. Forward pass through the model\n",
    "            2. Calculate loss\n",
    "            3. Zero gradients\n",
    "            4. Backward pass\n",
    "            5. Update parameters\n",
    "            \"\"\"\n",
    "            # YOUR CODE BELOW HERE\n",
    "            raise NotImplementedError(\"train_model_pytorch_rig_juice training loop is not implemented.\")\n",
    "            # YOUR CODE ABOVE HERE\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        loss = total_loss / len(train_loader)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if i % EVAL_EVERY == 0:\n",
    "            \"\"\"\n",
    "            Implement the evaluation loop steps:\n",
    "            1. Forward pass through the model (no need for gradients)\n",
    "            2. Calculate loss\n",
    "            3. Get predictions and compare with true labels\n",
    "            \"\"\"\n",
    "            # YOUR CODE BELOW HERE\n",
    "            raise NotImplementedError(\"train_model_pytorch_rig_juice evaluation loop is not implemented.\")\n",
    "            # YOUR CODE ABOVE HERE\n",
    "            val_losses.append(loss.item())\n",
    "            if plot_losses:\n",
    "                plot_loss(train_losses, val_losses, eval_interval=EVAL_EVERY, dynamic=True)\n",
    " \n",
    "    if plot_losses:\n",
    "        plot_loss(train_losses, val_losses, eval_interval=EVAL_EVERY, dynamic=False)\n",
    "    return model, criterion, optimizer, train_losses, val_losses, (test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rig_juice_model, criterion, optimizer, train_losses, val_losses, test_data = train_model_pytorch_rig_juice(features, labels, plot_losses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should achieve an accuracy of 95% in this portion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = test_data\n",
    "predictions = rig_juice_model(test_features)\n",
    "predictions = torch.argmax(predictions, dim=1)\n",
    "accuracy = (predictions == test_labels).sum().item() / len(test_labels)\n",
    "print(\"Accuracy: \", str(100 * accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xrspi1xOyWcB"
   },
   "source": [
    "# Grading\n",
    "\n",
    "You score for this part of the assignment will be out of **50 points**. A more detailed breakdown is as follows: \n",
    "- 5 points will be towards implementing the data loading functions correctly. \n",
    "- 15 points will be towards implementing the MNIST network in PyTorch correctly (achieving 95% accuracy).\n",
    "- 30 points will be towards implementing the Rig Juice network in PyTorch correctly (achieving 95% accuracy).\n",
    "\n",
    "**There will be no hidden tests for the MNIST network or the data loading functions.** Your score for that portion of the assignment will be visible when submitting to Gradescope.\n",
    "\n",
    "For the hidden tests for the Rig Juice dataset, we will use data from the same dataset provided (that is, you will not have to generalize your code to perform on a different dataset) that we have partitioned already. This is to ensure correctness of your implementation. The autograder will run a variety of tests for each section and assign scores, which will be visible after grades are published on Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrRHYKs_yZIU"
   },
   "source": [
    "# Submission\n",
    "Upload this notebook with the name `submission.ipynb` file to Gradescope. The autograder will **only** run successfully if your file is named this way. You must ensure that you have removed all print statements from **your** code, or the autograder may fail to run. Excessive print statements will also result in muddled test case outputs, which makes it more difficult to interpret your score.\n",
    "\n",
    "We've added appropriate comments to the top of certain cells for the autograder to export (`# export`). You do NOT have to do anything (e.g. remove print statements) to cells we have provided - anything related to those have been handled for you. You are responsible for ensuring your own code has no syntax errors or unnecessary print statements. You ***CANNOT*** modify the export comments at the top of the cells, or the autograder will fail to run on your submission.\n",
    "\n",
    "You should ***not*** add any cells that your code requires to the notebook when submitting. You're welcome to add any code as you need to extra cells when testing, but they will not be graded. Only the provided cells will be graded. As mentioned in the top of the notebook, **any helper functions that you add should be nested within the function that uses them.**\n",
    "\n",
    "If you encounter any issues with the autograder, please feel free to make a post on Ed Discussion. We highly recommend making a public post to clarify any questions, as it's likely that other students have the same questions as you! If you have a question that needs to be private, please make a private post.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CS-3600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
